---
title: 使用jdbc向MySQL数据库插入大量数据
date: 2013-11-18 00:00:00
tags: Database
---
今天碰到一个问题，要将一个500*50000=25000000的excel表放入数据库进行分析，采用了mysql数据库。为了避免一个表有太多字段，所以拆成了竖轴A，横轴B，AB对应以及对应的值的表C，这样在C表中大概就有25000000条记录。在使用Java/Groovy向mysql数据库插入这些数据的过程中碰到了一些问题。
java/groovy程序泡在2013 macbook air 13寸低配版+8G上。jvm的最大堆内存和初始堆内存都设置到512M以上,jdk 版本1.7，groovy 2.1
mysql使用官方版本，使用开发型配置，没有做其他任何优化或相关设置。


mysql在远程服务器上，服务器配置不高，但带宽是10M。


A, B, C表没有建立多余索引，只有主键和not null等基本约束，这是为了避免有索引对插入的影响。


首先，为了简单，将excel表另存为csv格式再由groovy读取，这也是因为数据可是很简单。

其次，最开始我是每次从CSV中读取一行，然后把这一行的500个数据插入C表中。这个时候，虽然使用了PrepareStatement 的addBatch, executeBatch功能，但是结果发现性能还是非常糟糕，一分钟1万条可能都没有，这个性能太糟糕了，我这还只是一个demo的数据而已。

然后网上搜了一下，发现好像是jdbc的mysql驱动默认是批处理无效的，需要在连接字符串上加上rewriteBatchedStatements=true 变成类似"jdbc:mysql://${host}:$port/$dbName?rewriteBatchedStatements=true"的样子。试了一下，果然速度提高了10倍不止，不明觉厉。

虽然速度提高很多，还是不够满意，不过目前也没什么其他好方法，目前这速度也可以完成任务，就先这么处理。


网上还看到使用mysqldump可以达到非常快的速度，有时间可以试一下。


发此文仅作记录分享，更深入的原因和处理方式有时间研究。